<appendix id="release-notes" xreflabel="release-notes">
	<title>Release Notes</title>


<section>
<title>Release 6.2 - changes from 6.1_sp1</title>

<section>
	<title>Enhancements and Bug Fixes</title>
	<para>
	Base: rocks run roll now supports host flag to specify the hostname
	</para>

	<para>
	Base: rocks add host verifies hostname is not an appliance name
	</para>

	<para>
	Base: fix reverse path filtering on centos 6.x
	</para>

	<para>
	Base: rocks set host bootflag now is applied to the local grub kickstart configuration
	</para>

	<para>
	Base: security fix in kickstart.cgi and setPexboot.cgi
	</para>

	<para>
	Base: rocks add appliance now supports specification of a distribution
	</para>

	<para>
	Base: 411-alter-handler properly handle corrupted timestamp file
	</para>

	<para>
	Base: creates rst files for roll commands
	</para>

	<para>
	Base: fix for MTU not applied when using VLAN
	</para>

	<para>
	Base: commands: fix report interface with IPMI interface 
	</para>

	<para>
	Base: do not propagate group with GID < 500 through 411
	</para>

	<para>
	Base: set host ip now accepts NULL
	</para>

	<para>
	KVM: now you can specify the vcpu type with attributes
	</para>

	<para>
	HPC: use the proper BTL with openmpi
	</para>

	<para>
	ZFS: updated to 0.6.1
	</para>

</section>
</section>




<section>
<title>Release 5.6/6.1_sp1 - changes from 5.6/6.1</title>

<section>
	<title>Enhancements and Bug Fixes</title>
	<para>
	Base: rocks yum repository is clean from errors (automatic dependecies/provides detection 
	has been disable on several packages, in particular: all the opt-perl-* rpms, foudation-git, 
	etc.)
	</para>
	<para>
	Base: /opt/rocks/lib/mysql is not included in the loader path so system mysql 
	uses system libmysqlclient.so and not rocks mysql libs (several other useless 
	entries were removed from the ld.so.conf).
	</para>
	<para>
	Base: JDK updated
	</para>
	<para>
	Base: grub boot timeout is now 10 seconds
	</para>
	<para>
	Base: improved script to fetch 441 private key after re-installation of a node
	</para>
	<para>
	Base: it is now possible to use a /etc/ssh/ssh_known_hosts.local to insert static 
	entries
	</para>
	<para>
	Base: rocks set host interface ip does not allow anymore invalid IP number
	</para>
	<para>
	Base, Ganglia, KVM: several fixes and improvement to the documentations
	</para>


	<para>
 	Base: Bugfix. record in rocks data base partitioning information
	</para>

	<para>
 	Base: rocks report host interface was failing with bonded interface
	</para>

	<para>
	Ganglia: rrd uses previous data retention policy (as in ganglia 3.2) to reduce 
	disk usage (ganglia 3.3 policy was to 39MB per node 
	http://marc.info/?l=npaci-rocks-discussion&amp;m=135850438309730). 
	</para>

</section>
</section>



<section>
<title>Release 5.5/6.1 - changes from 5.5/6.0</title>

<section>
	<title>New Features</title>

<itemizedlist>
	<listitem>
	<para> Host-based SSH authentication is now the default. This
        eliminates the requirement the users have password-less ssh keys
        and/or mounted home area on remote nodes   	
        </para>
	</listitem>


	<listitem>
	<para> Two-factor SSH authentication using Google Authenticator 
        Apps for Android and/or iPhone is supported for any/all users. Users
        can easily create a time-based verification codes that are used
        in addition to their usual password.
        </para>
	</listitem>

	<listitem>
	<para>  libqrencode.so and qrencode added to easily generate
        QR 2-dimensional codes.
        </para>
	</listitem>

	<listitem>
	<para>  New ZFS-linux roll to support the ZFS file system via the 
         <ulink url="http://zfsonlinux.org">ZFS on Linux</ulink>.  Does not
         support zfs as root file system, yet. 
        </para>
	</listitem>

	<listitem>
	<para>  New kernel-org roll to more easily support vanilla linux
         kernels from kernel.org.  
        </para>
	</listitem>

	<listitem>
	<para> Added Infiniband Support (Mellanox Hardware). Subnet manager 
        runs by default on frontend. Devices properly configured for users to
        run OpenMPI applications. 
        </para>
	</listitem>
</itemizedlist>
</section>

<section>
	<title>Enhancements and Bug Fixes</title>
	<para>
	Base: removed creation of md5sum of all rpm packages during rocks create distro 
	</para>
	<para>
	Base: now properly configure rsyslog on Rocks 6 (instead of syslog)
	</para>
	<para>
	Base: properly configure logrotate on Rocks 6
	</para>
	<para>
	Base: better handlying of "rocks run host" when running command on the local host
	</para>
	<para>
	Base: added suport for frontend with single partition
	</para>
	<para>
	Base: new documentation section on ipmi and insert-ethers command line 
	</para>
	<para>
	Base: support for GPT partition table on the compute nodes (disk bigger that 2TB)
	</para>

	<para>
 	Base: Bugfix. Secure Attributes resynchronized after reinstall/reboot. 
	</para>

	<para>
 	Base: Bugfix. Fixed "remove appliance" to raise an error if a host with
        that appliance type stil exists. 
	</para>

	<para>
 	Base: Bugfix. Support software RAID-1 for compute node / partitions	
	</para>

	<para>
 	Base: Bugfix. For automatic reinstalls, grub configuration modified to
        add ksdevice= directive
	</para>

	<para>
 	Base: Bugfix. Fixed permissions on file: /etc/security/ca/ca.cfg 
	</para>

	<para>
	Base/Kernel: Bugfix. Kernel roll can now be rebuilt if just the 
        Rocks-trimmed OS Roll is installed. Full CentOS distribution
        not required.
	</para>

	<para>
	Kernel: tracker client now checks received rpm with internal rpm md5sum 
	</para>

	<para>
	Ganglia: updated Ganglia to 3.3.7
	</para>


</section>
</section>



<section>
<title>Release 5.5/6.0 - changes from 5.4.3</title>

<section>
	<title>New Features</title>

<itemizedlist>
	<listitem>
	<para>Same Code base for CentOS 5 and CentOS 6 Systems</para>

	<para>While there are significant differences beteween CentOS 5 and 
        CentOS 6, Rocks now supports both from the same source code.  5 and 6
	Rolls are <emphasis> not interchangeable </emphasis>.
	</para>
	</listitem>

	<listitem>
	<para>KVM Roll (virtualization)for CentOS 6 64-bit Systems</para>

	<para>
	Xen is no longer supported by RedHat/CentOS 6 system and KVM (Kernel
	Virtual Machine) is only supported for 64-bit machines.  KVM is 
        hardware-only virtualization and requires both capability and
	BIOS to be enabled.
	</para>
	</listitem>

	<listitem>
	<para>Environment Modules</para>
	<para>  
	CentOS 6 natively supports environment modules instead of *-selector.
	For Rocks 5.5, the indentical version of modules is compiled and 
        included in the base roll.
	</para>
	</listitem>
	<listitem>
	<para> New support for Yum repositories</para>
	<para>
		Standard OS-defined Repositories are no longer removed, but
                are instead disabled. the command
		<computeroutput> yum repolist all</computeroutput>, describes
		configured repos. 
	</para>
	</listitem>
	<listitem>
	<para>Selectable Public and Private Networks at Install</para>
	<para>
		Users are no longer required to select eth0 as the private
                and eth1 as the public networks for frontend installation.
                Install screens have drop-down menus to select the detected
                interface.
	</para>
	</listitem>

	<listitem>
	<para>Support of BIOS-named Ethernet devices</para>
	<para>
		Version 6 kernels name Ethernet devices differently from
        Version 5. Devices may have names like p2p1, p35p2, and/or eth0. 
	If you prefer standard device names for 6, you must install a node
        using the standard kernel flag of <computeroutput>biosdevname=0</computeroutput>
	</para>
	</listitem>

	<listitem>
	<para>Support of Frontends with One Physical Interface</para>
	<para>
	The installer will configure a logical Ethernet device (e.g. eth0:0) if
	only one physical network device is detected during frontend
	installation.
	</para>
	</listitem>

	<listitem>
	<para>Creation of Upstart Scripts for CentOS 6</para>
	<para>
	CentOS 6 use upstart to define to boot characteristics
	</para>
	</listitem>

	<listitem>
	<para>New Command: rocks report post</para>
	<para>
	<computeroutput>rocks report post</computeroutput> will convert
	the post section of an xml file to an executable shell script.
	This is an advanced user feature.
	</para>
	</listitem>

	<listitem>
	<para>New Package: foundation-sqlite</para>
	<para>
	This implements the SQLite3 single file database. See Upgraded 
	Tracker.
	</para>
	</listitem>

</itemizedlist>
</section>

<section>
	<title>Enhancements and Bug Fixes</title>
	<para>
	Base: Updated anaconda 
	</para>
	<para>
	Base: Built against CentOS 5.8/6.2 with Updates as of May 8, 2012
	</para>
	<para>
	Base: Java Updated to Version 7 Update 3 
	</para>
	<para>
	Base:  Eclipse Moved to Java Roll and Updated to version 3.7.1
	</para>
	<para>
	Base: Profile scripts only add once to PATH variables 
	</para>
	<para>
	Base: <computeroutput>rocks run host localhost </computeroutput>
        performs a fork instead of an ssh. 
	</para>
	<para>
	Base: Utilize subprocess and hashlib for newer version of Python 
	</para>
	<para>
	Base: Improve performance of report host dhcpd by nearly a factor of
        10 on large clusters.
	</para>
	<para>
	Base:  Added Git RPM.
	</para>
	<para>
	Base:  Really fixed Zombie Processes in 411 service
	</para>
	<para>
	Base:  Patched foundation-python to remove a requirement on /usr/local/bin/python.
	</para>
	<para>
	Base:  Updated Version of M2Crypto to 0.21.1
	</para>
	<para>
	Base:  Better Bootstrapping support for building Rocks. Supports 
        "bootstrap0" which starts on a bare OS install. Use yum whenever
	possible in package installation.
	</para>
	<para>
	Base: Removed rebuild of Kudzu. CentOS fixed their internal 
        build problems. 
	</para>
	<para>
	Base: Only have 32-bit or 64-bit version of Java. Not both. Works
        around broken Oracle-supplied RPMs where only one can be installed. 
	</para>
	<para>
	Base: Fixed various graph ordering issues.
	</para>

	<para>
	HPC: Use environment modules. Set rocks-openmpi as default MPI
        Module 
	</para>
	<para>
	HPC: Added PVM RPMs for Version 6. PVM dropped from Native OS. 
	</para>
	<para>
	HPC: OpenMPI compiled with SGE integration 
	</para>
	<para>
	Ganglia: Updated profile.d scripts to only add once to PATH  
	</para>
	<para>
	Ganglia: rrdtool renamed to foundation-rrdtool to avoid conflicts with
        external reposistories. 
	</para>
	<para>
	Ganglia: Removed a Package Conflict with ganglia-pylib
	</para>
	<para>
	Ganglia: Insured that ganglia linkes with Rocks-compiled rrdtool 
	</para>
	<para>
	Java: Eclipse updated to 3.7.1 
	</para>
	<para>
	Java:  JBOSS updated to 6.1.0.  Version 5 was deprecated and 
        would not build on CentOS 6 
	</para>
	<para>
	Kernel: Rewrote the rocks tracker to use SQLite for storing hashes.
        Eliminated segmentation faults of older code. 
	</para>
	<para>
	Kernel: handle difference between syslog and rsyslog 
	</para>
	<para>
	Kernel:  build keyutils only on 5, not on 6. 
	</para>
	<para>
	SGE: Added Security Patched to Open Grid Scheduler v6.2 update 5p3
	</para>
	<para>
	SGE: Fix compilation to work on both 5.8 and 6.2 
	</para>
	<para>
	Bio: Many build fixes for Perl modules. 
	</para>
	<para>
	Bio: Extracted profile scripts to an RPM for easier management 
	</para>
	<para>
	Bio: mpi-blast compiled with the rocks-opemmpi module loaded 
	</para>
	<para>
	Bio: BioPerl now depends on Perl Roll
	</para>
	<para>
	Bio: BioPerl CPAN support utils updated
	</para>
	<para>
	Condor: Update Condor to v7.6.6
	</para>
	<para>
	Condor: No support for CentOS 6/32-bit.  
	</para>
	<para>
	Condor: Install simple condor test. 
	</para>
</section>
</section>

<section>
<title>Release 5.4.3 - changes from 5.4</title>

<section>
	<title>New Features</title>

<itemizedlist>
	<listitem>
	<para>Rocks Security</para>
	<para>  The root password supplied during the frontend
		installation is now used only for the root password of the
		frontend. Earlier, this password would be used for the MySQL
		database, Wordpress, and as root passwords for all compute
		nodes. All these services are now configured 
		randomly-generated passwords.
	</para>
	<para> To set the root passwords for individual backend nodes, the user
	can now use command line tool 
	<computeroutput>rocks add host sec_attr ... attr=root_pw</computeroutput> 
	to add a specific root password. 
	<computeroutput>rocks sync host sec_attr ... </computeroutput> for
	the password to be set.  Root passwd on the frontend is changed using
	the OS-supplied <computeroutput>passwd</computeroutput> command.
	 </para>
	<para>
	The rationale behind setting random root passwords for all backend nodes
	is that, if by some means, an attacker gained access to the root
	account of a backend node, and then the adversary could ran an 
	offline attack against the crypted version of the root password, 
	none of the other nodes would be compromised.
	</para>
	<para>We've introduced the concept of secure attributes in the
	database. Any attribute such as passwords, private keys, etc can be
	stored in the secure attributes table in the database. This table is
	locked down to be readable only by root, and no one else. It's contents
	are not transferred during kickstart, and can only be synced using
	<computeroutput>rocks sync host sec_attr</computeroutput>, 
	which will update the host specific secure
	attributes. Each attribute must also have a plugin associated with it,
	to specify the action to be performed on the backend nodes with the
	secure attribute as its data.</para></listitem>
	<listitem>
	<para>Changes to 411 infrastructure</para>
	<para>
	411 files can now only be requested by privileged accounts 
	on backend nodes.
	This is enforced by checking that 411 requests originate only
	from privileged ports. Iptables is used to filter out requests that
	come in from non-privileged ports.
	</para>
	<para>
	411 filters now support pre-send, filter, and post-receive functions.
	These functions are used to modify the contents of the 411 files being
	distributed, and to perform local system actions once the 411 files have
	been received and written to disk. These filters are present in the form
	of 411 plugins which are stored, modified, and enhanced on the frontend.
	</para>
	<para>
	The 411 shared key is now distributed outside the kickstart file. This is
	driven by the command line. The client makes an RPC request to the
	frontend, and the frontend transfers the 411 shared key out-of-band. It
	uses rocks command line tools to verify that the request is coming from
	within the cluster.
	</para>
	</listitem>
	<listitem><para>Introduction of the categories and indicies for
	resolving host specific properties in the database.</para>
	<para>
		With this release, we've laid the foundation for creating and
		using random categories of host groups.
	</para>
	<para>
		In the previous releases of rocks, the only categories available
		to system administrators by which they could group hosts were -
		Global category, Appliance category, OS category, and Hosts
		category.
	</para>
	<para>
		With this release, we've introduced the capability of being able
		to create user specified categories. Some examples are - rack
		category, where hosts are grouped by rack, a bio category -
		where hosts can be grouped by whether or not a set of hosts have
		the bio roll installed, etc.
	</para>
	<para>
		We've also introduced the concept of category resolution, where
		when resolving all the categories that a host can belong to, we
		can specify an chain of resolution. For example - we can state
		that compute-0-0 belongs to categories [global, linux, compute,
		rack0, bio]. In our resolution, we can state that we want the
		properties of the hosts to be picked up from [global, compute,
		rack0]. This way compute-0-0 picks up only the properties that
		are part of its resolution chain of categories.
	</para>
	<para>
	Since this feature is still prototypical, at the moment, it is used only
	internally for firewall commands and single resolution chain. Subsequent
	releases will apply the same technique to attributes and routes.
	</para>
	</listitem>
	<listitem><para>Changes to Firewall commands</para>
	<para>
		The rocks firewall commands now require the presence of a
		rulename for every iptables rule. These rules are then ordered
		lexicographically by rule name.
	</para>
	<para>
		The firewall command structure has some significant changes 
		to reflect
		the categories and indicies feature described in the previous
		bullet point. Please look at the rocks command line
		documentation for more details on how to run the rocks firewall
		commands.
	</para>
	</listitem>
	<listitem><para>Introduction of Perl and Python rolls</para>
	<para>
		Two new rolls have been added to Rocks.
	</para>
	<itemizedlist>
		<listitem>
		<para>Perl Roll - 
		The Perl roll contains Perl 5.14.1, and plenty of CPAN modules
		that are required for the application software in Rocks to
		function properly. This version of Perl in installed in
		<computeroutput>/opt/perl</computeroutput>.
		</para>
		</listitem>
		<listitem>
		<para>Python Roll - The version of Python that the core rocks
		utils depend on, is version 2.4.2. This is a rather dated
		version of Python. To provide users with the latest versions of
		python, we've create a Python roll which contains Python version
		2.7.2 and version 3.2.1. These are both installed in
		<computeroutput>/opt/python/</computeroutput>.
		</para>
		</listitem>
	</itemizedlist>
	</listitem>
</itemizedlist>
</section>

<section>
	<title>Enhancements and Bug Fixes</title>
	<para>
	Base: Updated anaconda to v11.1.2.224
	</para>
	<para>
	Base: Built against CentOS 5.6 with Updates as of August 7, 2011
	</para>
	<para>
	Base: database secured during installation. DB security now setup using
	script.
	</para>
	<para>
	Base: Only root can create tables in cluster db. Use the rocks.my.cnf
	file to connect to db as root, because the db is already secured by the
	time this xml file.
	</para>
	<para>
	Base: Default random root passwords for client nodes
	</para>
	<para>
	Base: Added a Development Appliance. This is a backend appliance 
	designed for building rolls without impacting the frontend. The 
	appliance uses the frontend yum repository by default but can be
	configured to use it own local repository for full isolation. 
	</para>
	<para>
	Base: Removed foundation-perl, cpan, and cpan-support. These have been
	moved to the Perl roll
	</para>
	<para>
	Base: Save the debug files from the ramdisk onto /root of the hard disk,
	so we can use them for post-install analysis.
	</para>
	<para>
	Base: Properly report disk partitions into the database for software
	RAID file systems. Increase installation speeds for clients with
	software RAID file systems.
	</para>
	<para>
	Base: 411 shared key is no longer transferred through kickstart.
	It is transferred through "rocks sync host sharedkey"
	411 configuration is now generated through "rocks report host config411"
	411 files are not transferred during kickstart. They are now transferred
	at first boot.
	</para>
	<para>
	Base: Unambiguous add host command. Previously it was not possible add
	hostnames that were command line actions. For example, you could not
	add a host called "attr" because the command "rocks add host attr" 
	exists. 
	</para>
	<para>
	Base: Remove shadow attributes from attributes tables and added secure
	attributes tables.
	</para>
	<para>
	Base: Set primary interface of login servers back to private. Otherwise
	SGE behaves badly.
	</para>
	<para>
	Base: Firewall rules now have rulenames and lexical ordering.
	</para>
	<para>
	Base: Add profile.d/ssh-keygen to login appliances
	</para>
	<para>
	Base: Hard link /etc/ssh/authorized_keys/id_rsa.pub to
	/root/.ssh/id_rsa.pub. If a user (or update) sets root's directory
	permissions tightly, we still read the public key.
	</para>
	<para>
	Base: Block non-priviledged traffic to 411 port from all networks,
	including localhost.
	</para>
	<para>
	Base: Add variable to manage number parallel instances of "rocks sync
	host" commands.
	</para>
	<para>
	Base: Support for pre-send, filtering, and post receive actions in 411.
	</para>
	<para>
	Base: Minor modification to 411put. Use a get_filename function instead
	of a filename constant.
	</para>
	<para>
	Base: 411 plugins can access host attributes
	</para>
	<para>
	Base: 411 filters user password, and shadow information of users with
	UID &lt; 500
	</para>
	<para>
	Base: Initial Support for host categories and indicies.
	</para>
	<para>
	Base: named.conf bug fixes - Now supports multiple subnets on non-octet
	boundaries.
	</para>
	<para>
	Base: If yum install fails due to dependency error, force install using
	rpm --nodeps.
	</para>
	<para>
	Base: Rocks run roll now honors the "--interpreter" flag to the
	post sections.
	</para>
	<para>
	Base: Honor .&lt;arch&gt; directive to yum install.
	When installing packages use,
	"yum install &lt;package&gt;" instead of "yum install
	&lt;packagefile&gt;.rpm"
	</para>
	<para>
	Base: Use YUM instead of RPM for rocks run roll
	This fixes two issues - On 64bit we were not installing the 32bit RPMs,
	and name.arch packages were not being installed.
	</para>
	<para>
	Base: Support for HVM when using the Xen roll.
	</para>
	<para>
	Base: Runtime optimization: Do not regenerate ALL pxeboot files. 
	Just those for the hosts
	specified on the command line.
	</para>
	<para>
	Base: Parallel class now takes care of serializing
	tasks that may overrun the system. If more than
	a set number of tasks are running, then requesting
	tasks will wait till slots are available to run
	Each task now prints out error messages if they fail
	on remote hosts. This way, we can track which syncs failed
	and which ones succeeded.
	</para>
	<para>
	Base: Root ssh key needs to be passwordless to allow command/sync access
	to backend nodes. If root, create ssh key without passphrase. If normal user, create
	key interactively.
	</para>
	<para>
	Base: Updated Java to 1.6 update 26
	</para>
	<para>
	Base: Support for versioned centrals
	</para>

	<para>
	HPC: Removed ganglia-web-frontend-addons from HPC roll
	</para>
	<para>
	HPC: Update IOZone to 3.397
	</para>
	<para>
	HPC: Update MPICH2 to v1.4
	</para>
	<para>
	HPC: Update OpenMPI to v1.4.3
	</para>
	<para>
	Ganglia: Updated to v3.2.0
	</para>
	<para>
	Ganglia: gmond.conf cleaned to support updated version. 
	</para>
	<para>
	Ganglia: Updated apr to v1.4.5
	</para>
	<para>
	Ganglia: Updated apr-utils to v1.3.13
	</para>
	<para>
	Java: Tomcat-connectors rpm bug fix. Now no longer generates conflicts
	when installing tomcat httpd configuration.
	</para>
	<para>
	SGE: Upgraded SGE to Open Grid Scheduler v6.2 update 5p2
	</para>
	<para>
	SGE: Move the login appliance configuration out of the SGE roll and into
	the Base roll.
	</para>
	<para>
	Web Server: Updated Wordpress to 3.1.3
	</para>
	<para>
	Web Server: Updated Rocks Theme for Wordpress.
	</para>
	<para>
	Web Server: Scrub root password from the installation and set the admin
	password to a string that never hashes.
	</para>
	<para>
	Web Server: Wordpress admin password can be reset only if valid admin
	email is supplied.
	</para>
	<para>
	Xen: Now support HVM as well as paravirtual instances
	</para>
	<para>
	Xen: no longer need 'rocks-create-vlan'
	</para>
	<para>
	Xen: added a report to create the xendomains configuration file
	</para>
	<para>
	Xen: save the CA key and CA certificate that are used to authenticate
	</para>
	<para>
	libvirt messages.
	Xen: touch /var/lock/subsys/xendomains in order to save running VMs
	</para>
	<para>
	Xen: Ability to put frontend on arbitrary vm-container and set its name
	</para>
	<para>
	Xen: Explicitly state default for virtualization type
	</para>
	<para>
	Area51: set the right attr for the default tripwire email address
	</para>
	<para>
	Area51: send tripwire reports to multiple recipients
	</para>
	<para>
	Bio: Login appliance gets Bio Roll
	</para>
	<para>
	Bio: Biopython now depends on Python Roll
	</para>
	<para>
	Bio: BioPython upgraded to v1.5.7
	</para>
	<para>
	Bio: BioPerl now depends on Perl Roll
	</para>
	<para>
	Bio: BioPerl CPAN support utils updated
	</para>
	<para>
	Bio: All BioPerl CPAN utils now built with cpan2dist
	</para>
	<para>
	Bio: Updated EMBOSS to v6.3.1
	</para>
	<para>
	Bio: Updated Autodock suite to v4.2.3
	</para>
	<para>
	Bio: Update CGView to v2.0
	</para>
	<para>
	Bio: Update fasta to v36.3.5a
	</para>
	<para>
	Bio: Update MrBayes to v3.1.2
	</para>
	<para>
	Bio: Update Blast to v2.2.25
	</para>
	<para>
	Bio: Update reportlab to v2.5
	</para>
	<para>
	Bio: Update t_coffee to v8.99
	</para>
	<para>
	Bio: Update WGS to v6.1
	</para>
	<para>
	Condor: Update Condor to v7.6.2
	</para>
	<para>
	Condor: rocks login appliance submits jobs to condor
	</para>
	<para>
	Condor: Experimental: Support submission to EC2
	</para>
	<para>
	Condor: Add RANK parameter
	</para>
</section>
</section>

<section>
<title>Release 5.4 - changes from 5.3</title>

<section>
	<title> New Features </title>

<itemizedlist>

<listitem>
	<para>Redesign of the Avalanche Installer.</para>

	<para>
	While observing the performance of the Avalanche Installer on a
	1000-node machine, it became obvious that we must reduce as much
	traffic to the frontend as possible.
	This led to replacing the python BitTorrent-based installer with a
	BitTorrent-inspired installer written in C.
	The C code allows us to put more files into the peer-to-peer network,
	most notably: product.img (160KB), stage2.img (108MB) and
	updates.img (98MB).
	</para>

	<para>
	To further reduce traffic to the frontend, the frontend now sends
	package predictions to installing nodes.
	When a node asks for a package, the tracker on the frontend sends
	a list of node addresses where that package can be found, plus a list of
	the next 9 packages that node will most likely ask for next.
	When similar appliances are concurrently installing, this reduces
	tracker traffic by 10x.
	</para>

	<para>
	Installing nodes can be grouped.
	When an installing node asks the tracker for the location of a package
	and if other nodes are concurrently installing, the tracker will
	favor nodes that are in the same group as the requesting node, that
	is, the list the tracker sends back to the installing node will have
	nodes from the same group as the installing node at the top of the list.
	The default grouping is by rack, but it can be controlled by the "coop"
	attribute.
	For example, if you would like to put all nodes from rack 0 and rack 1
	in the same group (named "red"), you would execute: "rocks set host
	attr rack0 rack1 coop red".
	</para>
	
	<para>
	One can specify multiple trackers and multiple "package servers".
	A package server is a node that is "guaranteed" to have the
	requested file (e.g., the frontend).
	</para>

	<para>
	For every downloaded file, an MD5 checksum verification is performed.
	This detects the case where a peer may have corrupted a file and 
	prevents the corrupted file from spreading into the
	peer-to-peer network.
	</para>
</listitem>

<listitem>
	<para>Channel bonding for nodes is now controlled by the Rocks command
	line.</para>

	<para>
	Channel bonding configuration for a node is stored in the database
	and can be added, removed or modified with the Rocks command line
	(e.g., "rocks add host bonded ...").
	After channel bonding is configured for a node, it can be dynamically
	applied by executing "rocks sync host network ...".
	</para>
</listitem>

<listitem>
	<para>All nodes' firewall rules are controlled by the Rocks command
	line.</para>

	<para>
	The rules for all the nodes are stored in the database and can be added,
	removed or modified with the Rocks command line (e.g., "rocks open
	host firewall", "rocks close host firewall", "rocks remove host
	firewall").
	After a node's firewall settings are changed, they can be applied to
	the node on-the-fly with "rocks sync host firewall 'hostname'"
	(this command is also called when the user executes
	"rocks sync host network ...").
	</para>
</listitem>

<listitem>
	<para>Introduction of "Air Traffic Control".</para>

	<para>
	We've developed a service known as the "Airboss" that resides on the
	physical frontend (in Dom0) and it allows non-root users to control
	their VMs.
	The motivation for this service is that libvirt (a virtualization API
	written by RedHat that can control several different virtualization
	implementations) assumes "root" access to control and monitor VMs.
	</para>

	<para>
	The Airboss in Rocks is a small service that uses digitally signed
	messages to give non-root users access to their virtual cluster (and
	only their virtual cluster).
	The Airboss relies upon public/private key pairs to validate messages.
	The administrator of the physical hosting cluster must issue a single
	command to associate a public key with a particular virtual cluster.
	At that point, the full process of powering up, powering down and
	installing a virtual
	cluster can be controlled by the (authorized) non-root user.
	</para>

	<para>
	In addition to VM power control, we've also added the ability to
	attach to a VM's console.
	This allows users to see the entire boot sequence for a VM starting
	from the "BIOS" boot messages.
	</para>

	<para>
	Several Rocks commands were added to support this feature:
	"rocks create keys" (to create public/private key pairs),
	"rocks set host power" (to power up/down VMs and to forcibly install
	a VM, akin to PXE booting a physical machine), and
	"rocks open host console" (to attach to a VM's console).
	</para>
</listitem>

<listitem>
	<para>"greceptor" replaced with "channeld".</para>

	<para>
	The wire protocol for Ganglia messages changed which required a
	major overhaul to greceptor. We made the decision to write a simple
	RPC-based service (named 'channeld') to take over the responsibilities
	of greceptor. Channeld accepts 411-put requests and acts on them by
	using 411-get to download files under the control of 411.
	</para>

	<para>
	All other components of 411 remain unchanged, only the notification
	engine has been enhanced.
	</para>
</listitem>

<listitem>
	<para>DNS resolution for multiple domains.</para>

	<para>
	The DNS naming system on the frontend now supports multiple zones,
	where each subnet managed by the frontend can be put into a different
	zone.
	The DNS service can be turned on or off for each individual zone.
	</para>
</listitem>

<listitem>
	<para>Login appliance support.</para>

	<para>
	A node can be configured as a Login appliance.
	By default, a Login appliance can submit jobs, but it cannot execute
	jobs.
	</para>
</listitem>

<listitem>
	<para>Set the name of a host based on the name of a specific
	network interface.</para>

	<para>
	The "primary_net" attribute allows nodes to have /bin/hostname set to
	the name of a network interface other than "private".
	This is useful for login or other multiple interface appliances.
	</para>
</listitem>

<listitem>
	<para>Easily swap 2 interfaces with one Rocks command.</para>

	<para>
	To swap the settings of 2 interfaces, execute "rocks swap host
	interface ...".
	</para>
</listitem>

<listitem>
	<para>Created a GIT repository for Rocks-related source code.</para>

	<para>
	The host "git.rocksclusters.org" is a GIT repository for all core
	Rocks code, UCSD Triton Resource code and Rocks contrib code.
	</para>
</listitem>

</itemizedlist>

</section>

<section>
	<title> Enhancements </title>

	<para>OS: Based on CentOS release 5/update 5 and all updates as of
	November 2, 2010.</para>

	<para>Base: Anaconda installer updated to v11.1.2.209.</para>

	<para>Base : no longer remap the private network to "eth0", instead
	Rocks keeps track of the network a node kickstarted from and maps that
	network to the "private" network. For example, if a node kickstarted
	off "eth1", then "eth1" will be mapped to the private network.</para>

	<para>Base : hardened the Anaconda installer to more aggressively write
	the grub configuration files onto the boot disk.
	This helps to mitigate the "hang while trying to load Grub stage2"
	issue.</para>

	<para>Base : removed ext4 kernel module from installation environment.
	We found that trying to mount a swap partition as an ext4 file system
	frequently caused kernel panics during installations.</para>

	<para>Base : added ksdevice=bootif to all the PXE boot targets.
	This improves installation speed by reusing the IP address/interface
	information when a node PXE boots.
	Previously, a node would re-scan all ethernet interfaces.</para>

	<para>Base : when a node XML file has a syntax error,
	"rocks list host profile" prints out the name of the node XML file and
	the line number where the syntax error occurred.</para>

	<para>Base : "rocks run host" now spawns multiple parallel threads when
	multiple hosts are supplied. Also added the following
	parameters: timeout (thanks Tim Carlson!), delay, stats, collate and
	num-threads.</para>

	<para>Base : yum configuration default modified to bind to the
	frontend's public IP instead of the private.
	This facilitates easy package installation for external nodes (e.g.,
	nodes running on a public cloud).</para>

	<para>Base : non-existent attributes are considered to be false
	conditionals when building configuration files.</para>

	<para>Base : "precedes" method added for Rocks command plugins to
	enable fine-grained ordering of plugin execution.</para>

	<para>Base : network interfaces under Linux support 2 new specific
	modes: "dhcp" and "noreport".
	The "dhcp" mode indicates that the interface should always DHCP to
	get its address.
	The "noreport" mode specifies that no "ifcfg-*" file should be
	written for the interface.
	If a mode is not specified for an interface, then Rocks will create
	an "ifcfg-*" file for the interface based on values set in the
	database (just like it did in the previous release).</para>

	<para>Base : IPMI now uses the interface channel column in the
	networks table to specify the baseboard controller
	channel number.</para>

	<para>Base : text inside "changelog" tags is now wrapped in CDATA to
	allow XML escape characters.
	This is only supported for node XML files found within Rolls (not
	for node XML files found under
	/export/rocks/install/site-profiles.</para>

	<para>Base : rolls can be built without a complete copy of the Rocks
	source code.
	They use the Rocks development environment found under
	/opt/rocks/share/devel on a frontend.</para>

	<para>Area51: tripwire updated to v2.4.2.</para>

	<para>Bio: refreshed CPAN modules.</para>
	<para>Bio: refreshed CPAN MPI-Blast.</para>
	<para>Bio: added Celera Whole Genome Sequence Assembler.</para>

	<para>Condor: updated to v7.4.4.</para>

	<para>Condor: automated Condor configuration completely retooled:
	1) the configuration is Rocks command based instead of standalone
	CondorConf tool,
        2) it supports dynamic update of any/all configurations on nodes,
        3) it uses Rocks command plugins to allow additional automated condor
	config (e.g., via plugin, it can turn on MPI support).</para>

	<para>Condor: supports a pool password (shared secret) for additional
	host verification.</para>

	<para>Condor: integrates with EC2 roll to extend Condor pools with
	EC2 Hosts.</para>

	<para>Condor: support added for port ranges to facilitate firewall
	configuration.</para>

	<para>Condor: local copy of Condor's manpages added to roll
	documents.</para>

	<para>Condor: support for updating Condor on nodes without
	re-installation (e.g., rocks run host "yum update condor" ; rocks sync
	host condor).</para>

	<para>Ganglia: monitor-core updated to v3.1.7.</para>

	<para>Ganglia: rrdtool updated to v1.4.4.</para>

	<para>Ganglia: the Ganglia Roll can now be added on-the-fly to an
	existing frontend.</para>

	<para>Ganglia: all nodes send out their metric metadata every 3
	minutes. In the past, when gmond was restarted on the frontend, it
	couldn't collect metrics from the nodes because it had no metadata
	from the nodes (and it didn't have a way to ask the nodes because the
	nodes are configured in "deaf" mode).</para>

	<para>HPC: iozone updated to v3.347.</para>
	<para>HPC: iperf updated to v2.0.5.</para>
	<para>HPC: MPICH2 updated to v1.2.1p1.</para>
	<para>HPC: OpenMPI updated to v1.4.3.</para>

	<para>HPC: rocks-openmpi is the default MPI and it is configured with
	mpi-selector.</para>

	<para>SGE: SGE updated to V62u5.</para>

	<para>SGE: any host can be configured to be an execution host by
	setting the host's "exec_host" and "sge" attributes to true and any
	host can become a submission host by setting the host's "submit_host"
	and "sge" attributes to true.</para>

	<para>Web-server: mediawiki updated to v1.16.0.</para>
	<para>Web-server: wordpress updated to v3.0.1.</para>

	<para>Xen: any node can how host Xen virtual machines. This is
	controlled with the "xen" attribute.</para>

	<para>Xen: set the power for all nodes in a virtual cluster (except
	the VM frontend) with one command ("rocks set cluster power ...").
	Power settings can be "on", "off" or "install" (turn on and force
	installation).</para>

	<para>Xen: allow virtual machines to define VLAN tagged interfaces.
	Previously, VLAN tagging was only supported for physical
	interfaces.</para>

</section>

<section>
	<title> Bug Fixes </title>

	<para>Base: non-root users can no longer see the encrypted passwords
	with 'rocks list host attr'. Hashed passwords are now stored in
	a 'shadow' column in the attribute tables.</para>

	<para>Base: the "%" in "rocks run host %" now returns all hosts.
	Thanks to Tom Rockwell for the fix.</para>

	<para>Base: If an ethernet switch sends out a DHCP request, the DHCP
	server no longer sends it the "filename" and "next server" in the
	DHCP response.
	This caused some switches not to properly load their firmware. More
	generally, this is controlled by the "kickstartable", "dhcp_filename"
	and "dhcp_nextserver" attributes.</para>

	<para>Base: "rocks set password" asks the user to confirm their new
	password.</para>

	<para>Base: when a node requests a kickstart file and if the frontend
	determines that the frontend is too "busy", the kickstarting node now
	correctly does a random backoff before re-requesting its kickstart
	file. Prior to this fix, a node would backoff for 30 seconds.</para>

	<para>Base: multiple conditionals can now be present in XML tags.</para>

	<para>Base: fixed a graph traversal issue. In the past, if you had the
	graph "a" (cond) to "b" to "c" and if "cond" was false, the graph
	traversal would include "a" and "c". Now it just includes "a".</para>

	<para>Base: permissions set in the "file" tag are preserved even if
	there are other "file" tags for the same file that don't set the file's
	permissions. The bug was when a later "file" tag without a "perms"
	attribute was encountered, the file's permissions were cleared.</para>

	<para>Base: "file" tags now support "os" conditionals.</para>

	<para>Base: in insert-ethers, appliances that are marked "not
	kickstartable" will not have to wait for a kickstart file. In the past,
	one had to hit the "F9" (force quit) key to exit insert-ethers when
	discovering non kickstartable appliances (e.g., ethernet switches).
	</para>

	<para>Base: IPMI configuration cleaned up.
	Rocks no longer generates erroneous entries in modprobe.conf or
	/etc/sysconfig/ifcfg-ipmi.</para>

	<para>Base: The "pre" tag now supports the "interpreter="
	attribute.</para>

	<para>Bio: eliminated "Permission Denied" errors during multiple runs
	on the same BLAST database by different users.</para>

	<para>SGE: made the job collection metric more efficient.
	Previously, when
	100's of jobs are submitted to a frontend's queue, the SGE metric
	would take so long to execute, it caused gmond to stop gathering
	metrics for all hosts.</para>

	<para>SGE: the number of CPUs array jobs consume are now correctly
	counted.</para>

</section>



</section>

<section>
<title>Release 5.3 - changes from 5.2</title>

<section>
	<title> Enhancements </title>

	<para>OS: Based on CentOS release 5/update 4 and all updates as of
	December 15, 2009.</para>

	<para>Area51: chkrootkit updated to v0.49.</para>

	<para>Base: Anaconda installer updated to v11.1.2.195.</para>

	<para>Base: Added support HP's Smart Array controllers
	(e.g., the 'cciss' driver) in the Rocks partitioning code.</para>

	<para>Base: Moved JDK to the Base Roll (the Java Roll has been
	discontinued).</para>

	<para>Base: Added the 'ssh_use_dns' attribute (default setting is
	'true'). If set to 'false', then DNS is not consulted when trying
	to make an SSH connection. This speeds up SSH connections for
	frontend's that don't have access to a functioning DNS server.
	</para>

	<para>Base: Added the 'Kickstart_DefaultLeaseTime' and
	'Kickstart_MaxLeaseTime' attributes to give the user the ability to
	control their own DHCP lease timeouts.
	</para>

	<para>Base: Added IMPI tools and configuration utilities.
	</para>

	<para>Base: Updated foundation-perl to v5.10.1.</para>

	<para>Base: Updated foundation-perl-Tk to v804.028.</para>

	<para>Base: Experimental CPAN support. This introduces a utility that
	lets the user create Rocks RPMs directly from CPAN along with all
	dependencies.
	</para>

	<para>Bio: Updated Emboss to v6.1.0.</para>

	<para>Bio: Updated biopython to v1.52.</para>

	<para>Bio: Updated clustalw to v2.0.12.</para>

	<para>Bio: Updated fasta to v35.4.9.</para>

	<para>Bio: Updated fftw to v3.2.2.</para>

	<para>Bio: Updated gromacs to v4.0.5.</para>

	<para>Bio: Updated iolib to v1.12.1.</para>

	<para>Bio: Updated phylip to v3.69.</para>

	<para>Bio: Updated t_coffee to v8.14.</para>

	<para>Ganglia: Updated rrdtool to v1.3.8.</para>

	<para>HPC: Updated iozone to v3.291.</para>

	<para>HPC: Updated iperf to v2.0.4.</para>

	<para>HPC: Updated MPICH2 to v1.1.1p1.</para>

	<para>HPC: Updated OpenMPI to v1.3.3.</para>

	<para>SGE: Updated to v6.2 update 4.</para>

	<para>Viz: Updated nVidia driver to v190.42.</para>

	<para>Web-server: Updated mediawiki to v1.15.1.</para>

	<para>Web-server: Updated wordpress to v2.8.4.</para>

</section>

<section>
	<title> Bug Fixes </title>

	<para>Area51: tripwire is now correctly configured on the
	frontend.</para>

	<para>Base: Increase the amount of memory PHP can use from 64 MB
	to 256 MB. This is essential for viewing Ganglia data for large
	clusters.</para>

	<para>Base: During installation, don't copy over the rocks-cdrom
	block device file. On most systems, there is no issue, but there are
	some systems where this code would copy over the entire contents of
	the CD/DVD if the CD/DVD was still in the tray (which is shouldn't be,
	because after the rolls are copied to the frontend, the CD/DVD is
	umounted and ejected).
	</para>

	<para>Base: Set the '--utc' flag when hwclock is called. Without the
	flag, the localtime is written to the hardware clock. Then, at first
	boot, the system reads the hardware clock respecting the UTC settings
	in /etc/sysconfig/clock. So the system time ends up being off by the
	offset to UTC.
	</para>

	<para>Base: On first boot, Set the number of CPUs for the frontend in
	the database to actual number of detected CPUs. In the past, this
	value was hard-coded to 1.
	</para>

	<para>Base: Fix for frontend's that use DNS servers that are configured
	to return a 'catch all' address for non-existant domain names. Do a
	reverse lookup to get the IP address. In the previous release, one
	would see an 'XXX' entry for the IP address of the frontend in
	/etc/hosts.
	</para>

	<para>Base: Put double quotes around 'option domain-name', otherwise,
	if the domain name was structured like xxx.yyy, then the DHCP service
	would not start.
	</para>

	<para>Base: For tentakel, throttle the number of concurrent
	connections to 100 (useful for large clusters).
	</para>

	<para>Base: Rewrite the pxelinux.cfg files after setting the
	run/install action in the nodes table (e.g., 'rocks set host
	installaction ...').
	</para>

	<para>Base: For 'rocks set password', make sure password changing
	code accesses the Rocks foundation database.
	</para>

	<para>Base: For 'rocks sync host network', make sure to update the
	static routes file.
	</para>

	<para>Base: Fixed 'insert-ethers --replace'. Before, it correctly
	removed the node, but then readded it at 'the end' of namespace,
	that is, it forgot the rack/rank of the replaced node.
	</para>

	<para>Base: Make tentakel more resilient to hanging nodes.
	</para>

	<para>Ganglia: Fix to save all RRD data on frontend shutdown.
	</para>

	<para>Ganglia: Fixes to physical views. Make sure all nodes are
	positioned in rack/rank order.
	</para>

	<para>Ganglia: Fixes to rediscover all backend nodes after a
	frontend reboot. Without this fix, if gmond was restarted on the
	frontend, then it requires restarting gmond on all the compute nodes
	in order to get metrics to report again.
	</para>

	<para>Kernel: Fixes to preserve the timestamp from RPMs on Rolls
	that are installed via CD/DVD. Prior to this fix, all RPMS in Rolls
	that were installed from CD/DVD would have the timestamp of the
	time when the frontend was installed. This causes issues when trying
	to update RPMs after the frontend was installed.
	</para>

	<para>SGE: When a host is removed (e.g., 'rocks remove host ...'),
	make sure the queue associated with that node is also removed.
	</para>

	<para>SGE: Build the DRMAA library with Java support.
	</para>

	<para>Xen: Respect the MTU value of the physical interface when
	bringing up Xen bridges.
	</para>

	<para>Xen: Fix to enable physical devices (partitions, LVM
	partitions, etc.) to be used as devices for virtual disks.
	</para>

	<para>Xen: Fix to 'ip' address flag in 'rocks add host vm'. Without
	the fix, if an IP address was specified on the command line, the
	command would throw an exception.
	</para>

	<para>Xen: Fix to support for virtual compute nodes that are managed
	by a physical frontend.
	</para>

</section>

</section>



<section>
<title>Release 5.2 - changes from 5.1</title>

<section>
	<title> New Features </title>

<itemizedlist>

<listitem>
	<para>Solaris support for client nodes</para>

	<para>
	With the new JumpStart Roll, one can now install and configure a
	Linux-based Rocks frontend to "JumpStart" Solaris-based back-end
	machines.
	</para>
</listitem>

<listitem>
	<para>Attributes</para>

	<para>
	Can assign "attributes" to nodes at four levels: global, appliance
	type, OS (e.g., Linux or SunOS), and host.
	An attribute can be accessed in an XML node as an entity.
	For example, if you assign the attribute "foo" with the value "123" 
	to compute-0-0 (i.e., with the command, "rocks set host attr foo 123"), 
	then in an XML node file, you can access the value of the attribute foo 
	with "&amp;foo;".
	</para>

	<para>
	Attributes also enable "conditionals".
	Using the example above, a "post" section can be optionally executed
	based on the value of an attribute.
	For example, if a post section is defined as:
	&lt;post cond="foo='123'"&gt; then this post section will only be
	executed
	if the attribute "foo" is set to "123" for the installing host.
	</para>
</listitem>

</itemizedlist>

</section>

<section>
	<title> Enhancements </title>

	<para>OS: Based on CentOS release 5/update 3 and all updates as of
	June 22, 2009.</para>

	<para>Base: Anaconda installer updated to v11.1.2.168.</para>

	<para>Base: Isolated MySQL for the Rocks database under
	/opt/rocks.</para>

	<para>Base: Converted all 'dbreports' to the Rocks command line.</para>

	<para>Base: Configure 'MTU' for networks through the Rocks command
	line.</para>

	<para>Base: Configure network routes through the Rocks command
	line.</para>

	<para>Base: Configure host aliases through the Rocks command
	line.</para>

	<para>Base: Added '/var/log/authpriv' to log rotate list.</para>

	<para>Base: Added 'iburst' flag to NTP configuration on frontend.</para>

	<para>Base: Added version and release info to vmlinuz and initrd.img
	to enable cross-kickstarting via PXE and making it easier to host
	different Rocks VMs on a physical system.</para>

	<para>Base: Added a YUM configuration file to each node that points
	to the distribution on the frontend.</para>

	<para>Bio: Updated biopython to v1.50.</para>

	<para>Bio: Updated clustalw to v2.0.11.</para>

	<para>Bio: Updated fasta to v35.4.7.</para>

	<para>Bio: Updated fftw to v3.2.1.</para>

	<para>Bio: Updated elph archive in glimmer to v1.0.1.</para>

	<para>Bio: Updated gromacs to v4.0.1.</para>

	<para>Bio: Updated perl-Data-Stag to v0.11.</para>

	<para>Bio: Updated perl-Digest-MD5 to v2.38.</para>

	<para>Bio: Updated perl-File-Temp to v0.21.</para>

	<para>Bio: Updated perl-GD to v2.41.</para>

	<para>Bio: Updated perl-GD-SVG to v0.33.</para>

	<para>Bio: Updated perl-Graph to v0.91.</para>

	<para>Bio: Updated perl-HTML-Parser to v3.60.</para>

	<para>Bio: Updated perl-HTML-Tagset to v3.20.</para>

	<para>Bio: Updated perl-PathTools to v3.30.</para>

	<para>Bio: Updated perl-SOAP-Lite to v0.710.08.</para>

	<para>Bio: Updated perl-SVG to v2.49.</para>

	<para>Bio: Updated perl-SVG-Graph to v0.02.</para>

	<para>Bio: Updated perl-Scalar-List-Utils to v1.21.</para>

	<para>Bio: Updated perl-Storable to v2.20.</para>

	<para>Bio: Updated perl-Text-Iconv to v1.7.</para>

	<para>Bio: Updated perl-URI to v1.38.</para>

	<para>Bio: Updated perl-XML-Parser to v2.36.</para>

	<para>Bio: Updated perl-XML-Twig to v3.32.</para>

	<para>Bio: Updated perl-XML-Writer to v0.606.</para>

	<para>Bio: Updated perl-bioperl to v1.6.0.</para>

	<para>Bio: Updated perl-libnet to v1.22.</para>

	<para>Bio: Updated perl-libwww-perl to v5.826.</para>

	<para>Bio: Updated phylip to v3.68.</para>

	<para>Bio: Updated reportlab to v2.3.</para>

	<para>Bio: Updated t_coffee to v7.81.</para>

	<para>Ganglia: Updated to v3.1.2.</para>

	<para>HPC: Updated OpenMPI to v1.3.2.</para>

	<para>HPC: Updated MPICH2 to v1.0.8p1.</para>

	<para>HPC: Updated stream to v5.9.</para>

	<para>Java: Updated java to v1.6.0_13.</para>

	<para>Java: Updated jdk to v6 update 13.</para>

	<para>Java: Updated antlr to v3.1.</para>

	<para>Java: Updated jboss to v5.0.1.GA.</para>

	<para>Java: Updated jogl to v1.1.1.</para>

	<para>SGE: Updated to v6.2 update 1.</para>

	<para>Viz: Support for single, dual and quad display nodes.</para>

	<para>Viz: Chromium support for 32-bit and 64-bit applications.</para>

	<para>Viz: Added CUDA driver. Users can optionally run CUDA programs
	on the tiled-display nodes.</para>

	<para>Viz: User updatable nVidia driver. Makes it easy for users to
	refresh the nVidia driver without having to wait for an updated Viz
	Roll.</para>

	<para>Xen: Using libvirt instead of 'xm' command line programs to
	start/stop VMs.</para>

	<para>Xen: Allow VM disk to be backed by a physical disk
	partition.</para>

	<para>Xen: Use threading in the 'rocks add cluster' command to
	decrease the time to add a virtual cluster.</para>
</section>

<section>
	<title> Bug Fixes </title>

	<para>Base: Fix for software RAID partitioning.</para>

	<para>Base: Increase timeout for package downloads when on slow networks (e.g., 100 or 10 Mbit).</para>

	<para>Xen: Fix to ensure all routes are active after Xen is
	started.</para>

	<para>Xen: Fix 'rocks set host vm' command to allow users to resize a
	VM's disk.</para>
</section>

</section>

<section>
<title>Release 5.1 - changes from 5.0</title>

<section>
	<title> New Features </title>

<itemizedlist>

<listitem>
	<para> Support for Virtual Clusters </para>

	<para>
	Virtual frontends and virtual compute nodes are now supported.
	The network for a VM frontend its VM compute nodes are contained within
	its own VLAN.
	</para>

	<para>
	A virtual cluster is added with
	"rocks add cluster fqdn=X ip=Y num-computes=Z".
	See "rocks add cluster help" for details.
	</para>
	
</listitem>

<listitem>
	<para> Can build rolls outside of Rocks source tree. </para>

	<para>
	All roll building support files are under
	<computeroutput>/opt/rocks/share/devel</computeroutput>.
	</para>
</listitem>

<listitem>
	<para>
	Can reconfigure a compute node's network without rebooting.
	</para>

	<para>
	Rocks commands were added to support this.
	See the documentation for the procedure.
	</para>
</listitem>

<listitem>
	<para>
	Distribution moved to
	<computeroutput>/export/rocks/install</computeroutput>.
	</para>

	<para>
	No longer require NFS on the frontend to properly host a Rocks
	Distribution.
	This will make moving user accounts to an external NFS server easier.
	</para>
</listitem>

<listitem>
	<para>
	Fine-grained control over the "boot" and "install" kernel for Xen
	VMs.
	</para>

	<para>
	Rocks commands where added to support this feature.
	For details, execute: "rocks help installprofile" and
	"rocks help bootprofile".
	</para>
</listitem>

</itemizedlist>

</section>

<section>
	<title> Enhancements </title>

	<para>
	OS: Based on CentOS release 5/update 2 and all updates as of
	November 4, 2008.
	</para>

	<para>
	Base: Anaconda installer updated to v11.1.2.113.
	</para>

	<para>
	Base: Increased the / partition default size to 16 GB.
	</para>

	<para>
	Base: Opened the 'www' and 'https' ports to the local public
	network.
	</para>

	<para>
	Base: In Avalanche Installer, added code to check if a package is
	requested twice in a row.
	If it is, we assume the package is corrupted.
	In this case, we toss the package and retrieve it the package from
	the frontend.
	</para>

	<para>
	Base: Added Rocks commands to manage the "aliases" table.
	</para>

	<para>
	Base: Added "rocks remove roll" command.
	Thanks to Brandon Davidson from the University of Oregon for the code.
	</para>

	<para>
	Base: The command "rocks-dist" is replaced with "rocks create distro".
	</para>

	<para>
	Base: Disabled the watchdog for frontend installs that boot off a
	CD/DVD.
	</para>

	<para>
	Base: Changed boot command from "frontend" to "build".
	To build a frontend, when you see the "boot:" prompt, now type:
	"build".
	</para>

	<para>
	Web Server: Wordpress updated to v2.6.1.
	</para>

	<para>
	Web Server: Updated Wordpress theme.
	</para>

	<para>
	Area51: All commands converted to Rocks command line.
	</para>

	<para>
	HPC: Updated OpenMPI to v1.2.7.
	</para>

	<para>
	HPC: Updated MPICH2 to v1.0.7.
	</para>

	<para>
	Java: Fixed a bug in the graph that Java from properly installing on
	compute nodes.
	</para>

	<para>
	Restore: All files under /export/rocks/install/contrib are now
	included in the Restore Roll.
	</para>

	<para>
	Restore: All files in /var/named/*local are now
	included in the Restore Roll.
	</para>

	<para>
	Restore: The frontend's ssh machine keys are now 
	included in the Restore Roll.
	</para>

	<para>
	SGE: Updated to v6.1 update 5.
	</para>

	<para>
	SGE: Added a script to reinstall a cluster by submitting an SGE job.
	</para>
</section>

<section>
	<title> Bug Fixes </title>

	<para>
	Base: Reverse domain lookups now work for subnets that don't fall
	on an octet boundary.
	</para>

	<para>
	Base: Bootflags now carry over between reinstallations and reboots.
	In the previous release, a reboot would "forget" the bootflags set
	by the user with the Rocks command line.
	</para>

	<para>
	Base: Added full path to "mksquashfs".
	Now can build distribution when using "sudo".
	</para>

	<para>
	Restore: Ethernet Switches, Power Units and Remote Management
	appliances are now properly saved in the Restore Roll.
	</para>

	<para>
	Ganglia: Fixed the 'tail +4' bug in the cron job.
	</para>

	<para>
	SGE: Fixed the display in the "Job Queue" on the frontend's web site.
	SGE now reports the correct number of CPUs in use.
	</para>

</section>
</section>

<section>
<title>Release 4.3 - changes from 4.2.1</title>

<section>
	<title> New Features </title>

<itemizedlist>

<listitem>
	<para> Rocks Command Line </para>

	<para> Initial release of the Rocks command line which
	facilitates non-SQL administrative access to the database. All Rocks
	commands have a regular structure of "rocks &lt;verb&gt;
	&lt;component&gt;". For example, to list all hosts that have been
	discovered by the frontend, execute: "rocks list host".
	</para>

	<para>
	All rocks commands can be listed by executing: rocks. Also, help is
	included with each command. For example, for help on the command
	"rocks add host", execute: "rocks add host help".
	</para>

	<para>
	For an overview of the Rocks commmand line, see
	<ulink url="http://www.rocksclusters.org/roll-documentation/base/4.3/commandline.html">Introduction to the Rocks Command Line</ulink>. The reference
	for all Rocks commands can be found <ulink url="http://www.rocksclusters.org/roll-documentation/base/4.3/c229.html">here</ulink>.
	</para>
</listitem>

<listitem>
	<para> PXE First </para>

	<para>
	Hosts can now be configured in BIOS with a boot order of CD, PXE,
	Hard Disk (previous releases of Rocks required: CD, Hard Disk, PXE).
	In combination with the Rocks command line, node-specific installation
	parameters are easily supported. For details on PXE First, see
	<ulink url="http://www.rocksclusters.org/roll-documentation/base/4.3/boot-order.html">Boot Order and PXE First</ulink>.
	</para>

	<para>
	Note: The boot order of (CD, HD, PXE) continues to be supported in
	Rocks 4.3. That is, existing Rocks clusters can be upgraded without
	requiring the cluster owner to change any BIOS settings.
	</para>
</listitem>

</itemizedlist>

</section>

<section>
	<title> Enhancements </title>

	<para>
	OS: Based on CentOS release 4/update 5 and all updates as of
	July 4, 2007.
	</para>

	<para>Base: Anaconda installer updated to v10.1.1.63.</para>

	<para>
	Base: Performance improvement when building torrent files for the
	Avalanche Installer.
	</para>

	<para>
	Base: Database indirects. More flexibility with Rocks variables.
	</para>

	<para>Grid: Globus updated to gt4.0.4 with web services.</para>

	<para>Condor: updated to v6.8.5.</para>

	<para>PVFS2: updated to v2.6.3.</para>

	<para>Java: updated to v1.5.0_10.</para>

	<para>Ganglia: updated to v3.0.4.</para>

	<para>HPC: Now using OpenMPI and PVM from RedHat distribution.</para>
</section>

<section>
	<title> Bug Fixes </title>

	<para>
	Base: Install now supports machines which have more than 26 disk
	drives.
	</para>

	<para>Base: 411 clients now atomically update files.</para>

	<para>
	Condor: Max heap size properly set for java programs on small and
	large memory machines.
	</para>

	<para>Condor: All logging written to /var/opt/condor.</para>
</section>
</section>

<section>
<title> Release 3.2.0 - changes from 3.1.0 </title>

	<para>
	New Feature - Added the Condor Roll.
	This brings the distributed high-throughput features from the
	Condor project to Rocks clusters.
	</para>

	<para>
	New Feature - Added the Area51 Roll.
	This roll contains security tools and services to check the
	integrity of the files and operating system on your cluster.
	</para>

	<para>
	New Feature - Ganglia RSS news event service.
	</para>

	<para>
	Enhancement - Improved network handling for compute nodes: any
	interface may be used for the cluster private network, not simply
	the default "eth0".
	</para>

	<para>
	Enhancement - Better support for cross-architecture clusters containing
	x86 and x86_64 machines. 
	</para>

	<para>
	Enhancement - GM device driver now builds and loads on compute nodes
	that have a custom kernel (e.g., a kernel from kernel.org).
	</para>

	<para>
	Enhancement - Software RAID for custom compute node partitioning
	is supported.
	</para>

	<para>
	Enhancement - Added variables for root and swap partition.
	If you only want to change the size of root and/or swap, you only
	have to reassign two XML variables.
	</para>

	<para>
	Enhancement - The default root partition size has been increased
	to 6 GB (up from 4 GB).
	</para>

	<para>
	Enhancement - SGE ganglia monitor added.
	The state of all SGE jobs can be tracked from the frontend's web page.
	</para>

	<para>
	Enhancement - PXE support extended to support floppy-based Etherboot and
	ia64.
	</para>

	<para>
	Enhancement - EKV uses ssh instead of telnet for security.
	</para>

	<para>
	Enhancement - New Myrinet MPICH version 1.2.5..12.
	</para>

	<para>
	Enhancement, Java Roll -- Updated JDK to version 1.4.2_04
	</para>

	<para>
	Enhancement - Latest software updates recompiled for three architectures
	from RHEL source rpms.
	</para>

	<para>
	Enhancement - Automatic MySQL Cluster database backup.
	</para>

	<para>
	Enhancement - MAC addresses are included for each node in the
	"Cluster Labels" output.
	</para>

	<para>
	Enhancement - Frontend rescue mode on the Rocks Base CD enabled.
	By typing "frontend rescue" at the boot prompt will give you
	a shell in which you can examine the state of the frontend.
	</para>

	<para>
	Bug Fix - 411 hardened. More reliable notification of changed files.
	Correct Makefile encrypts login files on frontend first-boot. 
	</para>

	<para>
	Bug Fix - Multiple CD drives are supported for bringing up a
	frontend.
	If you have more than one CD drive connected to your frontend,
	the installer will now correctly identify which CD you are using.
	</para>

	<para>
	Bug Fix - Ganglia metrics are now saved on frontend reboot.
	After a reboot, all Ganglia history will be restored from the
	previous boot.
	</para>

	<para>
	Bug Fix - PVFS compiled with -mcmodel=kernel on Opteron.
	</para>

	<para>
	Bug Fix - XML escape characters (e.g., &amp;, &lt;, &gt;) are
	supported in the installation screens (e.g., the Cluster Information
	screen and the Root Password screen).
	</para>

	<para>
	Bug Fix, Intel Roll - All the Intel compiler libraries are now copied
	to the compute nodes.
	</para>

</section>


<section>
<title>Release 3.2.0 - changes from 3.1.0</title>

	<para>
	New Feature - Added the Condor Roll.
	This brings the distributed high-throughput features from the
	Condor project to Rocks clusters.
	</para>

	<para>
	New Feature - Added the Area51 Roll.
	This roll contains security tools and services to check the
	integrity of the files and operating system on your cluster.
	</para>

	<para>
	New Feature - Ganglia RSS news event service.
	</para>

	<para>
	Enhancement - Improved network handling for compute nodes: any
	interface may be used for the cluster private network, not simply
	the default "eth0".
	</para>

	<para>
	Enhancement - Better support for cross-architecture clusters containing
	x86 and x86_64 machines. 
	</para>

	<para>
	Enhancement - GM device driver now builds and loads on compute nodes
	that have a custom kernel (e.g., a kernel from kernel.org).
	</para>

	<para>
	Enhancement - Software RAID for custom compute node partitioning
	is supported.
	</para>

	<para>
	Enhancement - Added variables for root and swap partition.
	If you only want to change the size of root and/or swap, you only
	have to reassign two XML variables.
	</para>

	<para>
	Enhancement - The default root partition size has been increased
	to 6 GB (up from 4 GB).
	</para>

	<para>
	Enhancement - SGE ganglia monitor added.
	The state of all SGE jobs can be tracked from the frontend's web page.
	</para>

	<para>
	Enhancement - PXE support extended to support floppy-based Etherboot and
	ia64.
	</para>

	<para>
	Enhancement - EKV uses ssh instead of telnet for security.
	</para>

	<para>
	Enhancement - New Myrinet MPICH version 1.2.5..12.
	</para>

	<para>
	Enhancement, Java Roll -- Updated JDK to version 1.4.2_04
	</para>

	<para>
	Enhancement - Latest software updates recompiled for three architectures
	from RHEL source rpms.
	</para>

	<para>
	Enhancement - Automatic MySQL Cluster database backup.
	</para>

	<para>
	Enhancement - MAC addresses are included for each node in the
	"Cluster Labels" output.
	</para>

	<para>
	Enhancement - Frontend rescue mode on the Rocks Base CD enabled.
	By typing "frontend rescue" at the boot prompt will give you
	a shell in which you can examine the state of the frontend.
	</para>

	<para>
	Bug Fix - 411 hardened. More reliable notification of changed files.
	Correct Makefile encrypts login files on frontend first-boot. 
	</para>

	<para>
	Bug Fix - Multiple CD drives are supported for bringing up a
	frontend.
	If you have more than one CD drive connected to your frontend,
	the installer will now correctly identify which CD you are using.
	</para>

	<para>
	Bug Fix - Ganglia metrics are now saved on frontend reboot.
	After a reboot, all Ganglia history will be restored from the
	previous boot.
	</para>

	<para>
	Bug Fix - PVFS compiled with -mcmodel=kernel on Opteron.
	</para>

	<para>
	Bug Fix - XML escape characters (e.g., &amp;, &lt;, &gt;) are
	supported in the installation screens (e.g., the Cluster Information
	screen and the Root Password screen).
	</para>

	<para>
	Bug Fix, Intel Roll - All the Intel compiler libraries are now copied
	to the compute nodes.
	</para>

</section>


<section>
<title>Release 3.1.0 - changes from 3.0.0</title>

	<para>
	Base Linux packages compiled from publicly available RedHat Enterprise Linux 3 Source (Advanced Workstation) for all architectures.
	</para>

	<para>
	Switched to Sun Grid Engine 5.3 as the default batch scheduling system. 
	</para>

	<para>
	More Rolls: NMI/Globus Release 4, Java, Condor, Intel compiler rolls available.
	</para>

	<para>
	New Architectures: Opteron (x86_64) receives first-class functionality.
	</para>

	<para>
	Enhancement - New MPICH version 1.2.5.2. More efficient MPD parallel job-launcher handling. MPICH2 included by default as well.
	</para>

	<para>
	Enhancement - Using latest Myrinet mpich-gm 2.0.8 for all architectures.
	</para>

	<para>
	Enhancement - Updated SSH version 3.7.1 with no login delay.
	</para>

	<para>
	Enhancement - 411 Secure Information Service used by default, replacing NIS.
	</para>

	<para>
	Enhancement - Greceptor replaces Gschedule to support mpdring, 411, cluster-top and others. Achieves an order of magnitude better performance than its predecessor.
	</para>
</section>


<section>
<title>Release 3.0.0 - changes from 2.3.2</title>

	<para>
	Based on RedHat 7.3 for x86 and RedHat Advanced Workstation 2.1 for
	ia64 (all packages recompiled from publicly available source).
	</para>

	<para>
	Enhancement - Includes RedHat updated RPMS (and recompiled
	SRPMs for ia64), as of September 3 2003.
	</para>

	<para>
	Enhancement - Includes kernel version 2.4.20-20.7 for x86
	and version 2.4.18e.37 for ia64.
	Installation environment includes all drivers from the above kernel
	packages.
	</para>

	<para>
	Enhancement - New full-featured DNS server and
	structured ".local" naming conventions within cluster.
	</para>

	<para>
	Enhancement - Linpack (<computeroutput>xhpl</computeroutput>) works
	out of the box for Pentium IV and Athlon.
	</para>

	<para>
	Enhancement - Added remove node feature to
	<computeroutput>insert-ethers</computeroutput>.
	</para>

	<para>
	Enhancement - New layout of all MPICH transports.
	See <computeroutput>/opt/mpich</computeroutput> on the frontend
	for the new directory structure.
	</para>

	<para>
	Enhancement - Add support for 'Rolls'.
	An x86 Rocks frontend install now requires two CDs: the Rocks Base 
	CD and the HPC Roll.
	An ia64 frontend still requires only one DVD.
	</para>

	<para>
	Enhancement - Added 'Grid' Roll.  This roll includes all
	packages from NMI R3.1, which includes Globus, the Simple
	Certificate Authority, and other packages.
	</para>

	<para>
	Enhancement - High-Performance, fault-tolerant MPD job launcher
	made available.
	Automatic MPD ring creation and healing via KAgreement-mpd
	protocol.
	(Currently in beta phase for this release)
	</para>

	<para>
	Enhancement - New 411 Secure Information Service to replace NIS. 
	(Currently in beta phase for this release)
	</para>

	<para>
	Enhancement - Latest Ganglia version 2.5.4 including better
	webfrontend speed and streamlined appearance, and more
	efficient network and disk metric handling.
	</para>

	<para>
	Enhancement - New PhpSysInfo page on compute nodes, available along
	with /proc link on Ganglia host view page.
	</para>

	<para>
	Enhancement - Ganglia command line tool has new --clustersize and
	--alive=host options.
	</para>

	<para>
	Enhancement - Kickstart graph now viewable from frontend web page.
	</para>

	<para>
	Enhancement - For kickstart graph files, new &lt;file&gt; tags made
	available,
	with owner="root.root" and perms="ga+r" attributes. Beta phase of 
	RCS-based tracking of all config file changes made for post-section 
	repeatability.
	</para>

	<para>
	Enhancement - Kickstart graph ordering is explicit.
	Previously the evaluation order of individual nodes depended
	on graph weights.  Node dependencies can now be explicitly
	specified using &lt;order&gt; tags in the graph files.
	</para>

	<para>
	Bug Fix - UNIX manual pages correctly shown (we extend /etc/man.conf)
	</para>

	<para>
	Bug Fix - NTP now synchronizes all compute node clocks with the
	frontend.
	</para>

	<para>
	Bug Fix - <computeroutput>add-extra-nic</computeroutput> now
	supports multiple NICs per compute node.
	</para>

	<para>
	Bug Fix - Ganglia RRD metric histories are archived on
	physical disk and restored on startup.
	</para>

	<para>
	Bug Fix - Includes NCSA's OpenPBS scalability patches.
	Can now launch PBS jobs that require more than 64 processors.
	</para>

	<para>
	Bug Fix - USB keyboard works on all ia64 Tiger boxes
	</para>

</section>




<section>
<title>Release 2.3.2 - changes from 2.3.1</title>

	<para>
	Bug fix - Memory leaks in the broadcastSSH gmetric python module are
	fixed.
	</para>

	<para>
	Bug fix - Gmetad will not crash when long ganglia metric names are
	introduced in the cluster.
	</para>

	<para>
	Bug Fix - Building MPICH-GM package correctly for AMD Athlon processors.
	</para>

	<para>
	Bug Fix - Added PBS directories:
	<computeroutput>/opt/OpenPBS/sched_priv</computeroutput>,
	<computeroutput>/opt/OpenPBS/sched_logs</computeroutput>,
	<computeroutput>/opt/OpenPBS/undelivered</computeroutput>.
	</para>

	<para>
	Bug Fix - Added <computeroutput>userdel</computeroutput> that
	correctly updates the NIS database.
	</para>

	<para>
	Enhancement - The Rocks-specific Ganglia metrics are much more
	efficient with a new Python C extension module that publishes
	ganglia metrics. The PBS job-queue monitor particularly benefits from
	this new module.
	</para>

	<para>
	Enhancement - Updated <computeroutput>rocks-boot</computeroutput>
	package to contain all the modules from the latest kernel-BOOT
	package.
	</para>

	<para>
	Enhancement - The Ganglia monitor-core and webfrontend packages have
	been updated to the latest version 2.5.3.
	</para>

	<para>
	Enhancement - The frontend is now a fully configured Rocks cluster
	build host.
	By checking out all the Rocks source code on a 2.3.2 frontend, one
	can build all the source code simply by executing
	<computeroutput>make rpm</computeroutput> in the directory
	<computeroutput>.../rocks/src/</computeroutput>.
	</para>

	<para>
	Enhancement - Updated SGE packages from v5.3p2-4 to v5.3p3-1.
	</para>

	<para>
	Enhancement - Added Rocks version number to
	<computeroutput>/home/install/contrib</computeroutput> directory
	structure.
	</para>
	
</section>



<section>
<title>Release 2.3.1 - changes from 2.3</title>

	<para>
	Bug fix - Now all the installation device drivers from Red Hat's
	device disks are included (e.g., Broadcom's Ethernet adapters).
	In Rocks 2.3, only the device drivers found on Red Hat's installation
	boot floppy were included.
	</para>

	<para>
	Bug fix - User-specified NIS domains are now supported (in
	Rocks 2.3, only 'rocks' NIS domain was supported).
	</para>

	<para>
	Bug fix - User-specified compute node disk partitioning is now
	supported.
	</para>

	<para>
	Bug fix - Sun Grid Engine commd port errors during post installation
	and Sun Grid Engine warnings during
	<computeroutput>insert-ethers</computeroutput> were fixed.
	</para>

	<para>
	Bug fix - Building for Pentium II/III and Athlon added to ATLAS RPM.
	(on a side note, ATLAS is now built against gcc version 3.2).
	</para>

	<para>
	Enhancement - PVFS upgraded to version 1.5.6.
	</para>

	<para>
	Enhancement - More detail has been added to the PBS queue monitoring
	web page (e.g., can view jobs for only one user and can view
	nodes for one job).
	Additionally, the monitoring code now more efficent and it has been
	hardened due to direct experiences on a 300-node Rocks cluster.
	</para>

	<para>
	Enhancement - The <computeroutput>bssh</computeroutput> service has
	been moved from a standalone service to a task managed by the
	Ganglia <computeroutput>gschedule</computeroutput> service.
	</para>

	<para>
	Enhancement - The ethernet-based MPICH package has been updated
	to version 1.2.5.
	</para>

	<para>
	Enhancement - The Myrinet-based MPICH package has been updated
	to version 1.2.5..9.
	</para>

	<para>
	Enhancement - <computeroutput>OpenPBS</computeroutput> version
	2.3.16 has replaced PBS.
	Additionally, the <emphasis>big memory</emphasis> patch has been
	applied.
	Also, the license for OpenPBS requires registration for those that
	use OpenPBS, so if you use OpenPBS to manage your computational
	resources, please register at http://www.OpenPBS.org.
	</para>

	<para>
	Enhancement - The <computeroutput>maui</computeroutput> package has
	been updated to version 3.2.5.
	</para>

	<para>
	Enhancement - Updated Myricom's GM to version 1.6.3.
	</para>

	<para>
	New Feature - Added a link of the main web page of the frontend
	that allows one to make sheets of labels with the names of all
	the compute nodes.
	</para>

	<para>
	New Feature - An alternative version of
	<computeroutput>gcc</computeroutput> is now installed (version 3.2
	is installed in /opt/gcc32/...).
	</para>

</section>



<section>
<title>Release 2.2.1 - changes from 2.2</title>


	<para>
	Bug fix - pvfs and gm modules don't build because the kernel source
	and kernel binary RPMs were of a different version.
	</para>

	<para>
	Bug fix - the partitioning on compute nodes only partitioned the
	first drive.
	Now all drives on compute nodes are partitioned with a single
	partition.
	The default partitioning is: 4 GB root partition, then /state/partition1
	is the remainder of the first drive. The second drive, if present,
	will have one partition labeled "/state/partition2".
	The third drive, if present, will have one partition labeled
	"/state/partition3", etc.
	</para>
 
	<para>
	Bug fix - the Rocks CD didn't support as many hardware devices as the
	RedHat CD.
	All the hardware modules found on the RedHat CD have been added to
	the Rocks CD (including many, many more).
	</para>

</section>


<section>
<title>Release 2.2 - changes from 2.1.2</title>

	<para>
	Based on RedHat 7.2.
	</para>

	<para>
	Upgraded Ganglia (provided by Matt Massie of UC Berkeley) to 2.1.1.
	</para>

	<para>
	Incorporated PVFS RPMs that were graciously provided to us
	from Najib Ninaba and Laurence Liew who work at
	Scalable Systems Pte Ltd in Singapore.
	</para>

	<para>
	insert-ethers looks to see if a Rocks distribution exists. If it
	doesn't, insert-ethers rebuilds it.
	</para>

	<para>
	Upgraded MPICH-GM to version 1.2.1..7b.
	</para>

	<para>
	Added the "stream" memory bandwidth benchmark.
	</para>

	<para>
	Added functionality to rocks-dist so distributions can be rebuilt
	without having to mirror the entire distribution.
	</para>

	<para>
	Implemented a "greedy" partitioning scheme on compute nodes. The
	default partitioning is: 4 GB root partition, then /state/partition1
	is the remainder of the first drive. The second drive, if present,
	will have one partition labeled "/state/partition2".
	The third drive, if present, will have one partition labeled
	"/state/partition3", etc.
	</para>

	<para>
	Bug fix - added a "watchdog" timer to kickstart. This reboots a
	kickstarting node if it can't find a kickstart file. This problem
	was reported by folks trying to kickstart multiple nodes at the
	same time.
	</para>

	<para>
	Bug fix - increased the polling intervals for maui so it won't time
	out when asking PBS about node status on larger clusters.
	</para>

	<para>
	Bug fix - makedhcp now adds the full pathname to pxelinux.0 when
	it builds dhcpd.conf.
	</para>

	<para>
	Bug fix - create a device node for /dev/cdrom.
	</para>

	<para>
	Bug fix - /var/log/messages is now appropriately rotated.
	</para>

</section>




<section>
<title>Release 2.1.2 - changes from 2.1.1</title>

	<para>
	Many network and storage drivers have been added to the installation
	CD. For example, SMC 83c170 EPIC/100 (epic100.o), RTL8139 SMC EZ Card
	Fast Ethernet (8139too.o) and the Promise SuperTrak Driver (pti_st.o)
	have all been included (as well as about 100 more).
	</para>

	<para>
	The cluster configuration web form has been simplified.
	</para>

	<para>
	The initial kickstart file that is generated from the web form is
	now streamed directly back to the user (rather than displaying the
	kickstart file, and then asking the user to save the file).
	This should finally kill the "I saved my kickstart file on Windows"
	problem.
	</para>

	<para>
	An option to manually partition a frontend disk has been
	added to the cluster configuration web form.
	</para>

	<para>
	The recursive directory /home/install/install/install/... has been
	eliminated.
	</para>

	<para>
	Ganglia's axon is now started before pbs-server, as the pbs-server
	initialization script asks ganglia for the number of processor in
	each node when it creates one of it's configuration files.
	</para>

	<para>
	The latest "stable" release of Myricom's GM (1.5) and MPICH-GM 
	(1.2.1..7) packages.
	</para>

	<para>
	High-Performance Linpack is now precompiled for Myrinet and Ethernet.
	</para>

</section>




<section>
<title>Release 2.1.1 - changes from 2.1</title>

	<para>
	The main change in this release is the use of an XML-based
	<emphasis>kickstart graph</emphasis> to actively manage kickstart files.
	</para>

	<para>
	Includes support for IA-64 compute nodes.
	See the 
	<ulink url="../howto/ia64.php">Installing IA-64 Compute
	Nodes HOWTO</ulink> for detailed information.
	</para>

	<para>
	A full X server is now installed on frontend machines.
	</para>

	<para>
	Added PXE support for kickstarting compute nodes.
	</para>

	<para>
	All compute nodes now install ATLAS and high-performance Linpack
	-- some slick software from the
	<ulink
	url="http://icl.cs.utk.edu/">Innovative Computing Laboratory</ulink>
	at the University of Tennessee.
	</para>

	<para>
	Modified to the PBS server initialization script to dynamically
	determine the number of CPUs in compute nodes by querying
	<computeroutput>ganglia</computeroutput>.
	</para>

	<para>
	Created a <computeroutput>rocks-pylib</computeroutput> package that
	contains all the common code used by Rocks command line utilties that
	access the MySQL database, thus giving all the tools the same basic
	functionality and common user-specified flags.
	</para>

	<para>
	Patched Red Hat's installation tool (anaconda) so the default
	behavior is to get kickstart files with HTTP (Red Hat's default
	is NFS).
	This frees the installation procedure of requiring NFS for
	<emphasis>any</emphasis> of its functions.
	</para>

	<para>
	Rewrite of <computeroutput>insert-ethers</computeroutput> to give it
	the look and feel of a standard Red Hat installation tool.
	</para>

	<para>
	Now using Red Hat's <computeroutput>pump</computeroutput> instead of
	<computeroutput>dhclient</computeroutput> for the DHCP client.
	</para>

	<para>
	Properly create the default PBS configuration file
	(<computeroutput>/usr/apps/pbs/pbs.default</computeroutput>) so PBS
	is now operational "out of the box".
	</para>

	<para>
	Fixed the annoying, but harmless, message
	<computeroutput>"socket.error: (101, 'Network is
	unreachable')"</computeroutput> that was seen on frontend boots.
	</para>

	<para>Fixed the annoying, but harmless, message
	<computeroutput>"user 0 unknown"</computeroutput> that was seen on a
	compute node's first boot after kickstarting.
	</para>

	<para>
	Fixed the 444 permissions problem on
	<computeroutput>/usr/man</computeroutput> and
	moved all the Rocks man pages into the new home for Linux man pages
	(<computeroutput>/usr/share/man</computeroutput>).
	</para>

</section>



<section>
<title>Release 2.1 - changes from 2.0.1</title>

	<para>
	The main change in this release is that thanks to RedHat 7.1, we
	now use the Linux 2.4 kernel.
	</para>
  
	<para>
	Based on RedHat 7.1, instead of 7.0.
	</para>

	<para>
	Linux 2.4.x kernel, instead of 2.2.x.
	</para>

	<para>
	Cluster-dist has been replaced with Rocks-dist.
	Command line arguments are very similar, with the
	<computeroutput>explode</computeroutput> command being
	removed and replaced with the
	<computeroutput>--copy</computeroutput> flag.
	The new Rocks-dist creates smaller distributions, fixes the problem
	of expensive mirror updating, and simplifies CD building.
	Also, it no longer deletes the distribution before rebuilding, this
	means the build directory (where kickstart files reside) is
	persistent across distribution builds.
	</para>

	<para>
	Frontend is now a stratum 10 NTP server, so compute nodes will
	clock sync to the frontend even when the frontend cannot reach an
	external time source.
	</para>

	<para>
	Usher daemon now correctly daemonizes, since we patch the GM
	code to allow processes to fork.
	</para>

	<para>
	Symbolic links for Ekv and piece-pipe RPMs removed from the
	build directory, and <computeroutput>"@Control@"</computeroutput>
	section added to kickstart files.
	</para>

	<para>
	Pbs_mom_config.h generated in the kickstart build directory.
	</para>

	<para>
	Added pre-defined types to the models table in the SQL database.
	Also, removed dead tables from database, and made column order more
	human friendly.
	</para>

	<para>
	Add SQL parsing to cluster-[ps|kill|fork] scripts.
	</para>

	<para>Removed cluster-config-compute, and cluster-config-frontend from
	the "%post" section in the kickstart file.
	The cluster-config rpm is now build and installed on the fly on each
	compute-node.
	</para>
      
	<para>
	Bumped lilo timeout to 5 seconds.
	</para>

	<para>
	Added FORCE_UNIPROCESSOR macro test to force sick SMP machines
	to kickstart as uniprocessor nodes.
	</para>

	<para>
	Major revision of insert-ethers.
	Can now be used to replace nodes, and start at arbitrary ranks and
	basenames.
	</para>

	<para>
	Minor maui and pbs bug fixes.
	</para>

	<para>
	Added gm-mpich SHMEM support to mpi-launch.
	</para>
</section>
   


<section>
<title>Release 2.0.1 - changes from 2.0</title>

	<para>
	Changed to new directory structure according to RedHat.
	Existing users will have to delete their mirror of www.rocksclusters.org
	and re-mirror to pickup the current RedHat directory naming scheme.
	NOTE: you need the new cluster-dist from www.rocksclusters.org to
	create a new mirror!
	</para>

	<para>
	Added support to kickstart laptops (still working on this)
	</para>

	<para>
	Frontend can now have either a DHCP or static address for the
	external network.
	For DHCP the DNS information provided from the
	external DHCP server is inserted into the Rocks Database and
	propagated to compute nodes.
	</para>

	<para>
	Increased default DHCP lease time
	</para>

	<para>
	Replaced Linux's useradd with create-account.
	</para>

	<para>
	Force glibc-common RPM to be installed.
	RedHat 7.0 doesn't install this due to errors in the RPM database.
	</para>

	<para>
	NIS database gets rebuilt on the frontend once an hour.
	</para>

	<para>
	Create directories on frontend/compute nodes before putting down
	SSL and SSH keys.
	Fixed permission on directories.
	</para>

	<para>
	Ssh-agent now forwards through nodes
	</para>

	<para>
	Ssh doesn't use privileged port (makes firewalls happy)
	</para>

	<para>cluster-kickstart set real and effect UID to root so all
	members of the install group can run shoot-node.
	Previously only root could do this.
	</para>

	<para>
	Fixed reinstalls on IDE and SCSI hosts (only IDA host worked before,
	thanks to a RedHat 7.0 change)
	</para>

	<para>
	Fixed bssh bug
	</para>

</section>

</appendix>
